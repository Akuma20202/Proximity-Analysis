{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Spleen Proximity project\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import monai\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import RocCurveDisplay, auc\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    CenterSpatialCrop,\n",
    "    EnsureChannelFirst,\n",
    "    RandRotate90,\n",
    "    Resize,\n",
    "    ResizeWithPadOrCrop,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    RandAdjustContrast,\n",
    "    RandShiftIntensity,\n",
    "    RandScaleIntensity,\n",
    "    RandHistogramShift,\n",
    "    ScaleIntensity,\n",
    "    ScaleIntensityRange,\n",
    "    AsDiscrete,\n",
    "    Activationsd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    Crop,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensity,\n",
    "    ScaleIntensityRanged,\n",
    "    ScaleIntensityRangePercentiles,\n",
    "    SpatialCrop,\n",
    "    ThresholdIntensity,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    "    Spacing\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch, ImageDataset, DistributedWeightedRandomSampler\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image count: 1216\n",
      "Label names: ['AAST13_old', 'AAST45_old', 'negative_old']\n",
      "Label counts: [455, 153, 608]\n"
     ]
    }
   ],
   "source": [
    "# path to dataset folder\n",
    "data_dir = '/home/mohammad/projects/ErrolProjects/spleen/dataNifti/abnormal/cropped_segmentor/'\n",
    "class_names = sorted(x for x in os.listdir(data_dir)\n",
    "                     if os.path.isdir(os.path.join(data_dir, x)))\n",
    "class_names = ['AAST13_old', 'AAST45_old', 'negative_old']\n",
    "num_class = len(class_names)\n",
    "image_files = [\n",
    "    [\n",
    "        os.path.join(data_dir, class_names[i], x)\n",
    "        for x in random.Random(0).sample(os.listdir(os.path.join(data_dir, class_names[i])), min(608, len(os.listdir(os.path.join(data_dir, class_names[i])))))\n",
    "    ]\n",
    "    for i in range(num_class)\n",
    "]\n",
    "\n",
    "num_each = [len(image_files[i]) for i in range(num_class)]\n",
    "image_files_list = []\n",
    "image_class = []\n",
    "for i in range(num_class):\n",
    "    image_files_list.extend(image_files[i])\n",
    "    image_class.extend([i] * num_each[i])\n",
    "num_total = len(image_class)\n",
    "\n",
    "\n",
    "print(f\"Total image count: {num_total}\")\n",
    "print(f\"Label names: {class_names}\")\n",
    "print(f\"Label counts: {num_each}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for roc k-fold\n",
    "n_splits = 5\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "input_patch_size = (128, 128, 48)\n",
    "batch_size = 1\n",
    "val_frac = 0.2\n",
    "test_frac = 0\n",
    "length = len(image_files_list)\n",
    "indices = np.arange(length)\n",
    "set_determinism(seed=0)\n",
    "np.random.shuffle(indices)\n",
    "spatial_dim = 3\n",
    "pretrained = False\n",
    "fold_r = {}\n",
    "cms = []\n",
    "n_splits = 5\n",
    "for fold in range(n_splits):\n",
    "    val_split = int(val_frac * length)\n",
    "    val_indices = indices[fold*val_split:(fold+1)*val_split]\n",
    "    val_x = [image_files_list[i] for i in val_indices]\n",
    "    val_y = [image_class[i] for i in val_indices]\n",
    "\n",
    "    val_transforms = Compose(\n",
    "        [\n",
    "            EnsureChannelFirst(),\n",
    "            ScaleIntensityRange(\n",
    "                a_min=-175,\n",
    "                a_max=250,\n",
    "                b_min=0.0,\n",
    "                b_max=1.0,\n",
    "                clip=True,\n",
    "            ),\n",
    "            Resize(input_patch_size),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "   # create a validation data loader\n",
    "    val_ds = ImageDataset(image_files=val_x, labels=val_y,\n",
    "                          transform=val_transforms)\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, sampler=None, batch_size=batch_size, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "    y_pred_trans = Compose([Activations(softmax=True)])\n",
    "    y_trans = Compose([AsDiscrete(to_onehot=num_class)])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = monai.networks.nets.densenet121(spatial_dims=spatial_dim, in_channels=1, pretrained=pretrained,\n",
    "                                            out_channels=num_class, init_features=128, dropout_prob=0).to(device)\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    val_interval = 1\n",
    "    auc_metric = ROCAUCMetric()\n",
    "    auc_metric.average = 'weighted'\n",
    "\n",
    "    # Test Time Augmentation\n",
    "    # from monai.data import TestTimeAugmentation\n",
    "\n",
    "    root_dir = '/home/mohammad/projects/ErrolProjects/spleen/'\n",
    "    best_metric = -1\n",
    "    best_acc = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "    best_model = f\"segmentor_normal_low_high_old_121_608samples_f{fold}.pth\"\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(root_dir, best_model)))\n",
    "    print(best_model)\n",
    "\n",
    "    model.eval()\n",
    "    classcount = np.bincount(val_y).tolist()\n",
    "    val_weights = 1./torch.tensor(classcount, dtype=torch.float)\n",
    "    val__sampleweights = val_weights[val_y]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor([], dtype=torch.long, device=device)\n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = (\n",
    "                val_data[0].to(device),\n",
    "                val_data[1].to(device),\n",
    "            )\n",
    "            y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
    "            y = torch.cat([y, val_labels.long()], dim=0)\n",
    "\n",
    "        acc_metric_balanced = balanced_accuracy_score(\n",
    "            np.array(y.cpu()), np.argmax(np.array(y_pred.cpu()), axis=1))\n",
    "\n",
    "        y_onehot = [y_trans(i) for i in decollate_batch(y)]\n",
    "        y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "        # ROC\n",
    "        y_score = [i.cpu()[0] for i in y_pred_act]\n",
    "\n",
    "        viz = RocCurveDisplay.from_predictions(\n",
    "            np.array(y.cpu()),\n",
    "            y_score,\n",
    "            pos_label=0, sample_weight=np.array(val__sampleweights.cpu()),\n",
    "            name=f\"ROC fold {fold}\",\n",
    "            alpha=0.6,\n",
    "            lw=2,\n",
    "            ax=ax,\n",
    "            plot_chance_level=(fold == n_splits - 1),\n",
    "        )\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "        auc_metric(y_pred_act, y_onehot)\n",
    "        result = auc_metric.aggregate()\n",
    "        auc_metric.reset()\n",
    "        metric_values.append(result)\n",
    "        acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "        acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "        \n",
    "        print(\n",
    "            f\" current AUC: {result:.4f}\"\n",
    "            f\" current accuracy: {acc_metric:.4f}\"\n",
    "            f'\\n balanced acc = {acc_metric_balanced}'\n",
    "        )\n",
    "\n",
    "        cm = confusion_matrix(np.array(y.cpu()), np.argmax(\n",
    "            np.array(y_pred.cpu()), axis=1))\n",
    "\n",
    "        # target_names = ['low-grade','high-grade','normal']\n",
    "        target_names = ['injured', 'normal']\n",
    "        cms.append(cm)\n",
    "\n",
    "\n",
    "        y_test = np.array(y.cpu())\n",
    "        y_score = np.argmax(np.array(y_pred.cpu()), axis=1)\n",
    "\n",
    "        fold_r[f'{fold}'] = multilabel_confusion_matrix(\n",
    "            np.array(y.cpu()), np.argmax(np.array(y_pred.cpu()), axis=1))\n",
    "        # break\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"Mean ROC curve with variability\\n(Positive label '{target_names[0]}')\",\n",
    ")\n",
    "ax.axis(\"square\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai",
   "language": "python",
   "name": "monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
